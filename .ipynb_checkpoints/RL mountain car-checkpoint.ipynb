{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning \n",
    "\n",
    "In this Python notebook, we will have you implement a simple reinforcement learning agent for the AI gym mountain car problem. Please first have a look at the description of the task here: <A HREF=\"https://github.com/openai/gym/wiki/MountainCar-v0\" TARGET=\"_blank\">Description</A>\n",
    "\n",
    "We will first experiment with the original formulation of the car mountain car task. The class we will use is CMC_original, which is the same as the normal AI gym version, but with an adapted render-function in order to be able to show the graphics in a Binder notebook.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random agent\n",
    "First run the following block, in which an agent is run that takes random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward = -200.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import run_cart\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class random_agent(object):\n",
    "    \"\"\"Random agent\"\"\"\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return random.randint(0,2)\n",
    "    \n",
    "agent = random_agent()\n",
    "reward, rewards = run_cart.run_cart_discrete(agent, env=run_cart.CMC_original(), graphics=True)\n",
    "print('Reward = ' + str(reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic agent\n",
    "Now let's try a heuristic agent, which uses a simple decision tree based on the position and velocity:\n",
    "\n",
    "NOTE: MAKE THIS AN ASSIGNMENT FOR THE STUDENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Success!\n",
      "Reward = -109.0\n"
     ]
    }
   ],
   "source": [
    "class heuristic_agent(object):\n",
    "    \"\"\"Guido's heuristic agent\"\"\"\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        position = observation[0]\n",
    "        velocity = observation[1]\n",
    "        if position < -0.5:\n",
    "            if velocity < -0.01:\n",
    "                action = 0\n",
    "            else:\n",
    "                action = 2\n",
    "        else:\n",
    "            if velocity > 0.01:\n",
    "                action = 2\n",
    "            else:\n",
    "                action = 0\n",
    "        return action\n",
    "    \n",
    "agent = heuristic_agent()\n",
    "reward, rewards = run_cart.run_cart_discrete(agent, env=run_cart.CMC_original(), graphics=True)\n",
    "print('Reward = ' + str(reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "Now all we need to do is have Q-learning find the simple heuristic by itself. We will start with the simplest form of tabular Q-learning in combination with the original mountain car task and reward function.\n",
    "\n",
    "TODO: HAVE THE STUDENTS FILL IN THE Q-UPDATE FUNCTION\n",
    "\n",
    "TODO: HAVE THE STUDENTS PLAY WITH THE LEARNING SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward per episode = -200.0\n",
      "Episode 231: Success!\n",
      "Episode 272: Success!\n",
      "Episode 294: Success!\n",
      "Episode 298: Success!\n",
      "Episode 338: Success!\n",
      "Episode 350: Success!\n",
      "Episode 351: Success!\n",
      "Episode 358: Success!\n",
      "Episode 377: Success!\n",
      "Episode 392: Success!\n",
      "Episode 394: Success!\n",
      "Episode 395: Success!\n",
      "Episode 396: Success!\n",
      "Episode 419: Success!\n",
      "Episode 423: Success!\n",
      "Episode 427: Success!\n",
      "Episode 428: Success!\n",
      "Episode 429: Success!\n",
      "Episode 430: Success!\n",
      "Episode 433: Success!\n",
      "Episode 434: Success!\n",
      "Episode 435: Success!\n",
      "Episode 436: Success!\n",
      "Episode 437: Success!\n",
      "Episode 438: Success!\n",
      "Episode 439: Success!\n",
      "Episode 440: Success!\n",
      "Episode 441: Success!\n",
      "Episode 446: Success!\n",
      "Episode 550: Success!\n",
      "Episode 551: Success!\n",
      "Episode 552: Success!\n",
      "Episode 553: Success!\n",
      "Episode 573: Success!\n",
      "Episode 612: Success!\n",
      "Episode 624: Success!\n",
      "Episode 625: Success!\n",
      "Episode 626: Success!\n",
      "Episode 627: Success!\n",
      "Episode 628: Success!\n",
      "Episode 629: Success!\n",
      "Episode 630: Success!\n",
      "Episode 631: Success!\n",
      "Episode 632: Success!\n",
      "Episode 633: Success!\n",
      "Episode 634: Success!\n",
      "Episode 635: Success!\n",
      "Episode 636: Success!\n",
      "Episode 638: Success!\n",
      "Episode 639: Success!\n",
      "Episode 640: Success!\n",
      "Episode 641: Success!\n",
      "Episode 642: Success!\n",
      "Episode 643: Success!\n",
      "Episode 644: Success!\n",
      "Episode 645: Success!\n",
      "Episode 646: Success!\n",
      "Episode 647: Success!\n",
      "Episode 648: Success!\n",
      "Episode 652: Success!\n",
      "Episode 674: Success!\n",
      "Episode 675: Success!\n",
      "Episode 682: Success!\n",
      "Episode 684: Success!\n",
      "Episode 685: Success!\n",
      "Episode 686: Success!\n",
      "Episode 690: Success!\n",
      "Episode 712: Success!\n",
      "Episode 762: Success!\n",
      "Episode 763: Success!\n",
      "Episode 766: Success!\n",
      "Episode 773: Success!\n",
      "Episode 774: Success!\n",
      "Episode 780: Success!\n",
      "Episode 808: Success!\n",
      "Episode 826: Success!\n",
      "Episode 833: Success!\n",
      "Episode 836: Success!\n",
      "Episode 846: Success!\n",
      "Episode 849: Success!\n",
      "Episode 850: Success!\n",
      "Episode 868: Success!\n",
      "Episode 873: Success!\n",
      "Episode 887: Success!\n",
      "Episode 889: Success!\n",
      "Episode 895: Success!\n",
      "Episode 900: Success!\n",
      "Episode 901: Success!\n",
      "Episode 928: Success!\n",
      "Episode 938: Success!\n",
      "Episode 953: Success!\n",
      "Episode 956: Success!\n",
      "Reward per episode = -196.47\n",
      "Episode 1: Success!\n",
      "Episode 6: Success!\n",
      "Episode 14: Success!\n",
      "Episode 15: Success!\n",
      "Episode 16: Success!\n",
      "Episode 19: Success!\n",
      "Episode 21: Success!\n",
      "Episode 29: Success!\n",
      "Episode 31: Success!\n",
      "Episode 32: Success!\n",
      "Episode 33: Success!\n",
      "Episode 46: Success!\n",
      "Episode 47: Success!\n",
      "Episode 51: Success!\n",
      "Episode 52: Success!\n",
      "Episode 56: Success!\n",
      "Episode 57: Success!\n",
      "Episode 58: Success!\n",
      "Episode 60: Success!\n",
      "Episode 63: Success!\n",
      "Episode 66: Success!\n",
      "Episode 68: Success!\n",
      "Episode 69: Success!\n",
      "Episode 71: Success!\n",
      "Episode 77: Success!\n",
      "Episode 78: Success!\n",
      "Episode 79: Success!\n",
      "Episode 80: Success!\n",
      "Episode 81: Success!\n",
      "Episode 82: Success!\n",
      "Episode 83: Success!\n",
      "Episode 84: Success!\n",
      "Episode 85: Success!\n",
      "Episode 86: Success!\n",
      "Episode 87: Success!\n",
      "Episode 89: Success!\n",
      "Episode 90: Success!\n",
      "Episode 91: Success!\n",
      "Episode 92: Success!\n",
      "Episode 93: Success!\n",
      "Episode 94: Success!\n",
      "Episode 95: Success!\n",
      "Episode 96: Success!\n",
      "Episode 97: Success!\n",
      "Episode 98: Success!\n",
      "Episode 99: Success!\n",
      "Reward per episode = -180.55\n"
     ]
    }
   ],
   "source": [
    "class Q_learning_agent(object):\n",
    "    \"\"\"Simple Q-learning agent for the MountainCarv0 task\n",
    "       https://en.wikipedia.org/wiki/Q-learning\n",
    "    \"\"\"\n",
    "\n",
    "    n_actions = 3\n",
    "\n",
    "    def __init__(self, min_speed, max_speed, min_position, max_position, alpha = 0.1, gamma = 0.9, p_explore = 0.1):\n",
    "        \n",
    "        # number of grids per state variable\n",
    "        self.n_grid = 10\n",
    "        self.min_speed = min_speed\n",
    "        self.max_speed = max_speed\n",
    "        self.speed_step = (max_speed - min_speed) / self.n_grid\n",
    "        self.min_position = min_position\n",
    "        self.max_position = max_position\n",
    "        self.position_step = (max_position - min_position) / self.n_grid\n",
    "        # discretizing the 2-variable state results in this number of states:\n",
    "        self.n_states = int(self.n_grid**2)\n",
    "        # make an empty Q-matrix\n",
    "        self.Q = np.zeros([self.n_states, self.n_actions])\n",
    "        #self.Q = np.random.rand(self.n_states, self.n_actions)\n",
    "        # initialize previous state and action\n",
    "        self.previous_state = 0\n",
    "        self.previous_action = 0\n",
    "        # learning rate\n",
    "        self.alpha = alpha\n",
    "        # discount factor:\n",
    "        self.gamma = gamma\n",
    "        # e-greedy, p_explore results in a random action:\n",
    "        self.p_explore = p_explore\n",
    "\n",
    "    def act(self, observation, reward, done, verbose = False):\n",
    "        \n",
    "        # Determine the new state:\n",
    "        pos = observation[0]\n",
    "        if(pos > self.max_position):\n",
    "            pos = self.max_position\n",
    "        elif(pos < self.min_position):\n",
    "            pos = self.min_position\n",
    "        obs_pos = int((pos - self.min_position) // self.position_step)                \n",
    "        vel = observation[1]\n",
    "        if(vel > self.max_speed):\n",
    "            vel = self.max_speed\n",
    "        elif(vel < self.min_speed):\n",
    "            vel = self.min_speed\n",
    "        obs_vel = int((vel - self.min_speed) // self.speed_step)\n",
    "        new_state = obs_pos * self.n_grid + obs_vel\n",
    "        \n",
    "        if(verbose):\n",
    "            print(f'Velocity {observation[1]}, position {observation[0]}, (grid {self.speed_step}, \\\n",
    "                          {self.position_step}), state = {new_state}')\n",
    "        \n",
    "        # Update the Q-matrix:\n",
    "        self.Q[self.previous_state, self.previous_action] +=  self.alpha * \\\n",
    "            (reward + self.gamma * max(self.Q[new_state, :]) - self.Q[self.previous_state, self.previous_action])\n",
    "        \n",
    "        # determine the new action:\n",
    "        if(random.random() < self.p_explore):\n",
    "            action = random.randint(0, self.n_actions-1)\n",
    "            #print(f'random action: {action:d}')\n",
    "        else:\n",
    "            action = np.argmax(self.Q[new_state, :])\n",
    "            #print(f'action: {action:d}')\n",
    "        \n",
    "        # update previous state and action\n",
    "        self.previous_state = new_state\n",
    "        self.previous_action = action        \n",
    "        \n",
    "        # return the action\n",
    "        return action\n",
    "\n",
    "    \n",
    "env=run_cart.CMC_original()\n",
    "\n",
    "# set up off-policy learning with p_explore = 1\n",
    "max_velocity = env.max_speed\n",
    "min_velocity = -max_velocity\n",
    "agent = Q_learning_agent(min_velocity, max_velocity, env.min_position, env.max_position, \\\n",
    "                         alpha = 0.20, gamma = 0.95, p_explore = 1.0)\n",
    "n_episodes = 1000\n",
    "reward, rewards = run_cart.run_cart_discrete(agent, env=env, graphics=False, n_episodes=n_episodes)\n",
    "print('Reward per episode = ' + str(reward / n_episodes))\n",
    "\n",
    "# on-policy now with e-greedy\n",
    "agent.p_explore = 0.05\n",
    "reward, rewards = run_cart.run_cart_discrete(agent, env=env, graphics=False, n_episodes=n_episodes)\n",
    "print('Reward per episode = ' + str(reward / n_episodes))\n",
    "\n",
    "n_episodes = 100\n",
    "agent.alpha = 0.05\n",
    "agent.p_explore = 0.02\n",
    "reward, rewards = run_cart.run_cart_discrete(agent, env=env, graphics=False, n_episodes=n_episodes)\n",
    "print('Reward per episode = ' + str(reward / n_episodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Success!\n",
      "Reward trained agent -142.0\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 1\n",
    "agent.p_explore = 0\n",
    "agent.alpha = 0\n",
    "reward, rewards = run_cart.run_cart_discrete(agent, env=env, graphics=True, n_episodes=n_episodes)\n",
    "print(f'Reward trained agent {reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
