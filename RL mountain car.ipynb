{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning \n",
    "\n",
    "In this Python notebook, we will have you implement a simple reinforcement learning agent for the AI gym mountain car problem. Please first have a look at the description of the task here: <A HREF=\"https://github.com/openai/gym/wiki/MountainCar-v0\" TARGET=\"_blank\">Description</A>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic agent\n",
    "Before we dive into the use of reinforcement learning of the mountain car task, you will first make a strategy by hand yourself. There are two very good reasons for doing so. \n",
    "\n",
    "First, making such a \"heuristic\" agent will give you a good idea of how difficult the task is, given the observations and actions of the agent. In the case of the mountain car task, the agent has to reach a goal position (located at position $0.5$), by choosing from discrete actions $\\{0,1,2\\}$ corresponding to $\\{$ push left, no acceleration, push right $\\}$. The starting position is in the middle, at position $-0.5$. The left end of the environment has position $-1.2$. The velocity of the agent is limited to the interval $[-0.07, 0.07]$.\n",
    "\n",
    "Second, making a heuristic agent gives you a good idea of the difficulty of the task, and what kind of solution you might expect the machine learning method to find (reinforcement learning in the current notebook).\n",
    "\n",
    "We will experiment with the original formulation of the car mountain car task. The class we will use is `CMC_original`, which is the same as the normal AI gym version, but with an adapted render-function in order to be able to show the graphics in a Binder notebook. The full code, imported with `import run_cart` in the code block below. You can find it <A HREF=\"https://github.com/guidoAI/RL_cart_notebook/blob/master/run_cart.py\" TARGET=\"_blank\">here</A>. \n",
    "\n",
    "<FONT COLOR=\"red\">Exercise 1.</FONT>\n",
    "<OL>\n",
    "    <LI>First run the following block, in which an agent is run that takes random actions. Does it succeed in performing the task? Why so? </LI>\n",
    "    <LI>If we want the agent to move to the right, why then not just drive there? Try this strategy out by replacing the random action in the `act` function with 2, which means to just accelerate to the right. What do you observe, and why?</LI>\n",
    "    <LI>Write a heuristic method to solve the task. Only make use of the position and velocity in the current time step, as this is what we are going to give to the reinforcement learning agent. Are you able to solve the task? Is your solution optimal you think? Do you think that this will be easy to learn with reinforcement learning?</LI>\n",
    "</OL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward = -200.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import run_cart\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# definition of the agent class\n",
    "class heuristic_agent(object):\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        \"\"\" - observation[0] = position\n",
    "            - observation[1] = velocity\n",
    "            - reward = the reward received from the environment\n",
    "            - done = whether the agent reached the goal\n",
    "        \"\"\"\n",
    "        position = observation[0]\n",
    "        velocity = observation[1]\n",
    "        \n",
    "        # REPLACE THIS CODE WITH A FIXED ACTION OR YOUR HEURISTIC:\n",
    "        action = random.randint(0,2)\n",
    "        \n",
    "        return action\n",
    "\n",
    "# create the agent:\n",
    "agent = heuristic_agent()\n",
    "# apply the agent to the task\n",
    "reward, rewards = run_cart.run_cart_discrete(agent, env=run_cart.CMC_original(), graphics=True)\n",
    "print('Reward = ' + str(reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "For this notebook, you will implement a very elementary reinforcement learning method: Q-learning. Please first read up on this method <A HREF=\"https://en.wikipedia.org/wiki/Q-learning\" TARGET=\"_blank\">here</A>. In particular, we will investigate the use of _tabular_ Q-learning, in which the algorithm finds the values for all state-action pairs in the matrix. Since our state is two-dimensional (position and velocity), the state-action space can be discretized without leading to an overly large table. In the code block below, we create the table in the `__init__` function of `Q_learning_agent`, using 10 grid cells per state variable. Also have a look at the start of the `act` function. When we get the two-dimensional state (observation), we transform it to a single state number (row index in the Q state-action table). The table has three columns for the three actions.\n",
    "\n",
    "<FONT COLOR=\"red\">Exercise 2.</FONT>\n",
    "<OL>\n",
    "    <LI>Fill in the Q-update function in the `act` function. Remember that variables of the agent object have to be reffered to with `self`  like `self.alpha` and `self.Q`.</LI>\n",
    "    <LI>Write the code to take actions under an $\\epsilon$-greedy policy, where $\\epsilon$ is `p_explore` in the code.\n",
    "    </LI>\n",
    "    <LI>Run the code to have the agent learn. Is it able to do the task (to check this, run the code block two blocks below in order to run the agent without exploratory actions)? Why / why not? If it does not work, can you then change the code at the bottom to make the agent learn the task properly?</LI>\n",
    "</OL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_learning_agent(object):\n",
    "    \"\"\"Simple Q-learning agent for the MountainCarv0 task\n",
    "       https://en.wikipedia.org/wiki/Q-learning\n",
    "    \"\"\"\n",
    "\n",
    "    n_actions = 3\n",
    "\n",
    "    def __init__(self, min_speed, max_speed, min_position, max_position, alpha = 0.1, gamma = 0.9, p_explore = 0.1):\n",
    "        \n",
    "        # number of grids per state variable\n",
    "        self.n_grid = 10\n",
    "        self.min_speed = min_speed\n",
    "        self.max_speed = max_speed\n",
    "        self.speed_step = (max_speed - min_speed) / self.n_grid\n",
    "        self.min_position = min_position\n",
    "        self.max_position = max_position\n",
    "        self.position_step = (max_position - min_position) / self.n_grid\n",
    "        # discretizing the 2-variable state results in this number of states:\n",
    "        self.n_states = int(self.n_grid**2)\n",
    "        # make an empty Q-matrix\n",
    "        self.Q = np.zeros([self.n_states, self.n_actions])\n",
    "        # initialize previous state and action\n",
    "        self.previous_state = 0\n",
    "        self.previous_action = 0\n",
    "        # learning rate\n",
    "        self.alpha = alpha\n",
    "        # discount factor:\n",
    "        self.gamma = gamma\n",
    "        # e-greedy, p_explore results in a random action:\n",
    "        self.p_explore = p_explore\n",
    "\n",
    "    def act(self, observation, reward, done, verbose = False):\n",
    "        \n",
    "        # Determine the new state:\n",
    "        pos = observation[0]\n",
    "        if(pos > self.max_position):\n",
    "            pos = self.max_position\n",
    "        elif(pos < self.min_position):\n",
    "            pos = self.min_position\n",
    "        obs_pos = int((pos - self.min_position) // self.position_step)                \n",
    "        vel = observation[1]\n",
    "        if(vel > self.max_speed):\n",
    "            vel = self.max_speed\n",
    "        elif(vel < self.min_speed):\n",
    "            vel = self.min_speed\n",
    "        obs_vel = int((vel - self.min_speed) // self.speed_step)\n",
    "        new_state = obs_pos * self.n_grid + obs_vel\n",
    "        \n",
    "        if(verbose):\n",
    "            print(f'Velocity {observation[1]}, position {observation[0]}, (grid {self.speed_step}, \\\n",
    "                          {self.position_step}), state = {new_state}')\n",
    "        \n",
    "        # ************************************************\n",
    "        # FILL IN YOUR CODE HERE FOR THE Q-UPDATE FUNCTION\n",
    "        # ************************************************\n",
    "        \n",
    "        # Update the Q-matrix:\n",
    "        # self.Q[..., ...] =  ...\n",
    "        \n",
    "        # determine the new action:\n",
    "        # FILL IN YOUR CODE FOR SELECTING AN ACTION\n",
    "        \n",
    "        # update previous state and action\n",
    "        self.previous_state = new_state\n",
    "        self.previous_action = action        \n",
    "        \n",
    "        # return the action\n",
    "        return action\n",
    "\n",
    "    \n",
    "env=run_cart.CMC_original()\n",
    "\n",
    "# set up off-policy learning with p_explore = 1\n",
    "max_velocity = env.max_speed\n",
    "min_velocity = -max_velocity\n",
    "agent = Q_learning_agent(min_velocity, max_velocity, env.min_position, env.max_position, \\\n",
    "                         alpha = 0.20, gamma = 0.95, p_explore = 1.0)\n",
    "n_episodes = 1000\n",
    "reward, rewards = run_cart.run_cart_discrete(agent, env=env, graphics=False, n_episodes=n_episodes)\n",
    "print('Reward per episode = ' + str(reward / n_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1\n",
    "agent.p_explore = 0\n",
    "agent.alpha = 0\n",
    "reward, rewards = run_cart.run_cart_discrete(agent, env=env, graphics=True, n_episodes=n_episodes)\n",
    "print(f'Reward trained agent {reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers\n",
    "\n",
    "<FONT COLOR=\"red\">Exercise 1.</FONT>\n",
    "<OL>\n",
    "    <LI>It typically does not succeed. For example, just when it speeds up to the right it then randomly accelerates to the left, and will not reach the goal.</LI>\n",
    "    <LI>The agent just never gets high enough on the up-slope. The exerted force is not sufficient by itself to counter gravity. The agent will likely have to swing up and down to gain more and more speed, which will finally bring it over the right hill.</LI>\n",
    "    <LI>The following code block has a solution to the task consisting of a few if / else statements. It always succeeds when starting at the start location. Whether it is the optimal solution is not clear, but it is clear that the state space can be separated quite easily and linked to the appropriate actions. It seems that the task is relatively easy to perform and learning algorithms should be able to do it.</LI>\n",
    "</OL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class heuristic_agent(object):\n",
    "    \"\"\"Guido's heuristic agent\"\"\"\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        position = observation[0]\n",
    "        velocity = observation[1]\n",
    "        if position < -0.5:\n",
    "            if velocity < -0.01:\n",
    "                action = 0\n",
    "            else:\n",
    "                action = 2\n",
    "        else:\n",
    "            if velocity > 0.01:\n",
    "                action = 2\n",
    "            else:\n",
    "                action = 0\n",
    "        return action\n",
    "    \n",
    "agent = heuristic_agent()\n",
    "reward, rewards = run_cart.run_cart_discrete(agent, env=run_cart.CMC_original(), graphics=True)\n",
    "print('Reward = ' + str(reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<FONT COLOR=\"red\">Exercise 2.</FONT>\n",
    "<OL>\n",
    "    <LI>The code for the update equation is as follows: \n",
    "        `self.Q[self.previous_state, self.previous_action] +=  self.alpha *  (reward + self.gamma * max(self.Q[new_state, :]) - self.Q[self.previous_state, self.previous_action])`</LI>\n",
    "    <LI>A possible implementation is:\n",
    "        `if(random.random() < self.p_explore): action = random.randint(0, self.n_actions-1) else: action = np.argmax(self.Q[new_state, :])`</LI>\n",
    "    <LI>In order to solve it, I made a learning scheme in three parts: off-policy learning with random actions, on-policy learning with less exploration, and on-policy learning with a lower learning rate and even less exploration.</LI>\n",
    "</OL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_learning_agent(object):\n",
    "    \"\"\"Simple Q-learning agent for the MountainCarv0 task\n",
    "       https://en.wikipedia.org/wiki/Q-learning\n",
    "    \"\"\"\n",
    "\n",
    "    n_actions = 3\n",
    "\n",
    "    def __init__(self, min_speed, max_speed, min_position, max_position, alpha = 0.1, gamma = 0.9, p_explore = 0.1):\n",
    "        \n",
    "        # number of grids per state variable\n",
    "        self.n_grid = 10\n",
    "        self.min_speed = min_speed\n",
    "        self.max_speed = max_speed\n",
    "        self.speed_step = (max_speed - min_speed) / self.n_grid\n",
    "        self.min_position = min_position\n",
    "        self.max_position = max_position\n",
    "        self.position_step = (max_position - min_position) / self.n_grid\n",
    "        # discretizing the 2-variable state results in this number of states:\n",
    "        self.n_states = int(self.n_grid**2)\n",
    "        # make an empty Q-matrix\n",
    "        self.Q = np.zeros([self.n_states, self.n_actions])\n",
    "        # initialize previous state and action\n",
    "        self.previous_state = 0\n",
    "        self.previous_action = 0\n",
    "        # learning rate\n",
    "        self.alpha = alpha\n",
    "        # discount factor:\n",
    "        self.gamma = gamma\n",
    "        # e-greedy, p_explore results in a random action:\n",
    "        self.p_explore = p_explore\n",
    "\n",
    "    def act(self, observation, reward, done, verbose = False):\n",
    "        \n",
    "        # Determine the new state:\n",
    "        pos = observation[0]\n",
    "        if(pos > self.max_position):\n",
    "            pos = self.max_position\n",
    "        elif(pos < self.min_position):\n",
    "            pos = self.min_position\n",
    "        obs_pos = int((pos - self.min_position) // self.position_step)                \n",
    "        vel = observation[1]\n",
    "        if(vel > self.max_speed):\n",
    "            vel = self.max_speed\n",
    "        elif(vel < self.min_speed):\n",
    "            vel = self.min_speed\n",
    "        obs_vel = int((vel - self.min_speed) // self.speed_step)\n",
    "        new_state = obs_pos * self.n_grid + obs_vel\n",
    "        \n",
    "        if(verbose):\n",
    "            print(f'Velocity {observation[1]}, position {observation[0]}, (grid {self.speed_step}, \\\n",
    "                          {self.position_step}), state = {new_state}')\n",
    "        \n",
    "        # Update the Q-matrix:\n",
    "        self.Q[self.previous_state, self.previous_action] +=  self.alpha * \\\n",
    "            (reward + self.gamma * max(self.Q[new_state, :]) - self.Q[self.previous_state, self.previous_action])\n",
    "        \n",
    "        # determine the new action:\n",
    "        if(random.random() < self.p_explore):\n",
    "            action = random.randint(0, self.n_actions-1)\n",
    "        else:\n",
    "            action = np.argmax(self.Q[new_state, :])\n",
    "        \n",
    "        # update previous state and action\n",
    "        self.previous_state = new_state\n",
    "        self.previous_action = action        \n",
    "        \n",
    "        # return the action\n",
    "        return action\n",
    "\n",
    "    \n",
    "env=run_cart.CMC_original()\n",
    "\n",
    "# set up off-policy learning with p_explore = 1\n",
    "max_velocity = env.max_speed\n",
    "min_velocity = -max_velocity\n",
    "agent = Q_learning_agent(min_velocity, max_velocity, env.min_position, env.max_position, \\\n",
    "                         alpha = 0.20, gamma = 0.95, p_explore = 1.0)\n",
    "n_episodes = 1000\n",
    "reward, rewards = run_cart.run_cart_discrete(agent, env=env, graphics=False, n_episodes=n_episodes)\n",
    "print('Reward per episode = ' + str(reward / n_episodes))\n",
    "\n",
    "# on-policy now with e-greedy\n",
    "agent.p_explore = 0.05\n",
    "reward, rewards = run_cart.run_cart_discrete(agent, env=env, graphics=False, n_episodes=n_episodes)\n",
    "print('Reward per episode = ' + str(reward / n_episodes))\n",
    "\n",
    "n_episodes = 100\n",
    "agent.alpha = 0.05\n",
    "agent.p_explore = 0.02\n",
    "reward, rewards = run_cart.run_cart_discrete(agent, env=env, graphics=False, n_episodes=n_episodes)\n",
    "print('Reward per episode = ' + str(reward / n_episodes))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
