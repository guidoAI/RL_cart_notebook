{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning \n",
    "\n",
    "In this Python notebook, we will have you implement a simple reinforcement learning agent for the AI gym mountain car problem. Please first have a look at the description of the task here: <A HREF=\"https://github.com/openai/gym/wiki/MountainCar-v0\" TARGET=\"_blank\">Description</A>\n",
    "\n",
    "We will first experiment with the original formulation of the car mountain car task. The class we will use is CMC_original, which is the same as the normal AI gym version, but with an adapted render-function in order to be able to show the graphics in a Binder notebook.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random agent\n",
    "First run the following block, in which an agent is run that takes random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import run_cart\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class random_agent(object):\n",
    "    \"\"\"Random agent\"\"\"\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return random.randint(0,2)\n",
    "    \n",
    "agent = random_agent()\n",
    "reward, rewards = run_cart.run_cart_discrete(agent, env=run_cart.CMC_original(), graphics=True)\n",
    "print('Reward = ' + str(reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic agent\n",
    "Now let's try a heuristic agent, which uses a simple decision tree based on the position and velocity:\n",
    "\n",
    "NOTE: MAKE THIS AN ASSIGNMENT FOR THE STUDENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class heuristic_agent(object):\n",
    "    \"\"\"Guido's heuristic agent\"\"\"\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        position = observation[0]\n",
    "        velocity = observation[1]\n",
    "        if position < -0.5:\n",
    "            if velocity < -0.01:\n",
    "                action = 0\n",
    "            else:\n",
    "                action = 2\n",
    "        else:\n",
    "            if velocity > 0.01:\n",
    "                action = 2\n",
    "            else:\n",
    "                action = 0\n",
    "        return action\n",
    "    \n",
    "agent = heuristic_agent()\n",
    "reward, rewards = run_cart.run_cart_discrete(agent, env=run_cart.CMC_original(), graphics=True)\n",
    "print('Reward = ' + str(reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "Now all we need to do is have Q-learning find the simple heuristic by itself. We will start with the simplest form of tabular Q-learning in combination with the original mountain car task and reward function.\n",
    "\n",
    "TODO: HAVE THE STUDENTS FILL IN THE Q-UPDATE FUNCTION\n",
    "\n",
    "TODO: HAVE THE STUDENTS PLAY WITH THE LEARNING SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_learning_agent(object):\n",
    "    \"\"\"Simple Q-learning agent for the MountainCarv0 task\n",
    "       https://en.wikipedia.org/wiki/Q-learning\n",
    "    \"\"\"\n",
    "\n",
    "    n_actions = 3\n",
    "\n",
    "    def __init__(self, min_speed, max_speed, min_position, max_position, alpha = 0.1, gamma = 0.9, p_explore = 0.1):\n",
    "        \n",
    "        # number of grids per state variable\n",
    "        self.n_grid = 10\n",
    "        self.min_speed = min_speed\n",
    "        self.max_speed = max_speed\n",
    "        self.speed_step = (max_speed - min_speed) / self.n_grid\n",
    "        self.min_position = min_position\n",
    "        self.max_position = max_position\n",
    "        self.position_step = (max_position - min_position) / self.n_grid\n",
    "        # discretizing the 2-variable state results in this number of states:\n",
    "        self.n_states = int(self.n_grid**2)\n",
    "        # make an empty Q-matrix\n",
    "        self.Q = np.zeros([self.n_states, self.n_actions])\n",
    "        #self.Q = np.random.rand(self.n_states, self.n_actions)\n",
    "        # initialize previous state and action\n",
    "        self.previous_state = 0\n",
    "        self.previous_action = 0\n",
    "        # learning rate\n",
    "        self.alpha = alpha\n",
    "        # discount factor:\n",
    "        self.gamma = gamma\n",
    "        # e-greedy, p_explore results in a random action:\n",
    "        self.p_explore = p_explore\n",
    "\n",
    "    def act(self, observation, reward, done, verbose = False):\n",
    "        \n",
    "        # Determine the new state:\n",
    "        pos = observation[0]\n",
    "        if(pos > self.max_position):\n",
    "            pos = self.max_position\n",
    "        elif(pos < self.min_position):\n",
    "            pos = self.min_position\n",
    "        obs_pos = int((pos - self.min_position) // self.position_step)                \n",
    "        vel = observation[1]\n",
    "        if(vel > self.max_speed):\n",
    "            vel = self.max_speed\n",
    "        elif(vel < self.min_speed):\n",
    "            vel = self.min_speed\n",
    "        obs_vel = int((vel - self.min_speed) // self.speed_step)\n",
    "        new_state = obs_pos * self.n_grid + obs_vel\n",
    "        \n",
    "        if(verbose):\n",
    "            print(f'Velocity {observation[1]}, position {observation[0]}, (grid {self.speed_step}, \\\n",
    "                          {self.position_step}), state = {new_state}')\n",
    "        \n",
    "        # Update the Q-matrix:\n",
    "        self.Q[self.previous_state, self.previous_action] +=  self.alpha * \\\n",
    "            (reward + self.gamma * max(self.Q[new_state, :]) - self.Q[self.previous_state, self.previous_action])\n",
    "        \n",
    "        # determine the new action:\n",
    "        if(random.random() < self.p_explore):\n",
    "            action = random.randint(0, self.n_actions-1)\n",
    "            #print(f'random action: {action:d}')\n",
    "        else:\n",
    "            action = np.argmax(self.Q[new_state, :])\n",
    "            #print(f'action: {action:d}')\n",
    "        \n",
    "        # update previous state and action\n",
    "        self.previous_state = new_state\n",
    "        self.previous_action = action        \n",
    "        \n",
    "        # return the action\n",
    "        return action\n",
    "\n",
    "    \n",
    "env=run_cart.CMC_original()\n",
    "\n",
    "# set up off-policy learning with p_explore = 1\n",
    "max_velocity = env.max_speed\n",
    "min_velocity = -max_velocity\n",
    "agent = Q_learning_agent(min_velocity, max_velocity, env.min_position, env.max_position, \\\n",
    "                         alpha = 0.20, gamma = 0.95, p_explore = 1.0)\n",
    "n_episodes = 1000\n",
    "reward, rewards = run_cart.run_cart_discrete(agent, env=env, graphics=False, n_episodes=n_episodes)\n",
    "print('Reward per episode = ' + str(reward / n_episodes))\n",
    "\n",
    "# on-policy now with e-greedy\n",
    "agent.p_explore = 0.05\n",
    "reward, rewards = run_cart.run_cart_discrete(agent, env=env, graphics=False, n_episodes=n_episodes)\n",
    "print('Reward per episode = ' + str(reward / n_episodes))\n",
    "\n",
    "n_episodes = 100\n",
    "agent.alpha = 0.05\n",
    "agent.p_explore = 0.02\n",
    "reward, rewards = run_cart.run_cart_discrete(agent, env=env, graphics=False, n_episodes=n_episodes)\n",
    "print('Reward per episode = ' + str(reward / n_episodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1\n",
    "agent.p_explore = 0\n",
    "agent.alpha = 0\n",
    "reward, rewards = run_cart.run_cart_discrete(agent, env=env, graphics=True, n_episodes=n_episodes)\n",
    "print(f'Reward trained agent {reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
